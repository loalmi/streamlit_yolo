# -*- coding: utf-8 -*-
import streamlit as st
import os
import cv2
import numpy as np
import tempfile
import torch
from ultralytics import YOLO
from PIL import Image
from datetime import datetime
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase

# –í–∞–∂–Ω–æ! –°–∞–º—ã–π –ø–µ—Ä–≤—ã–π Streamlit-–≤—ã–∑–æ–≤
st.set_page_config(page_title="–î–µ—Ç–µ–∫—Ü–∏—è —Å –¥—Ä–æ–Ω–∞", layout="wide")
st.title("–î–µ—Ç–µ–∫—Ü–∏—è –ª—é–¥–µ–π –Ω–∞ —Ñ–æ—Ç–æ, –≤–∏–¥–µ–æ –∏ —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ YOLOv5
@st.cache_resource
def load_model():
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
    model.eval()
    return model


model = load_model()

option = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫:", ["–§–æ—Ç–æ", "–í–∏–¥–µ–æ", "–í–µ–±-–∫–∞–º–µ—Ä–∞"])

# –§–æ—Ç–æ
if option == "–§–æ—Ç–æ":
    uploaded_image = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type=["jpg", "jpeg", "png"])
    if uploaded_image:
        image = Image.open(uploaded_image).convert("RGB")
        st.image(image, caption='–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', use_container_width=True)

        st.write("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        results = model(image)

        st.image(np.squeeze(results.render()), caption="–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏", use_container_width=True)

        df = results.pandas().xyxy[0]
        st.write("–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏:")
        st.dataframe(df)

# –í–∏–¥–µ–æ
elif option == "–í–∏–¥–µ–æ":
    uploaded_video = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ", type=["mp4", "mov", "avi"])
    if uploaded_video:
        st.video(uploaded_video)

        input_tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
        input_tmp.write(uploaded_video.read())
        input_tmp_path = input_tmp.name

        cap = cv2.VideoCapture(input_tmp_path)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS) or 24
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')

        now = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(tempfile.gettempdir(), f"detected_{now}.mp4")
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        st.write("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ...")
        frame_count = 0
        max_frames = 300

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or frame_count > max_frames:
                break
            results = model(frame)
            annotated = np.squeeze(results.render())
            out.write(cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))
            frame_count += 1

        cap.release()
        out.release()

        st.success("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        st.video(output_path)

        with open(output_path, "rb") as video_file:
            st.download_button("üì• –°–∫–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ", video_file, file_name=f"detected_{now}.mp4")

# –í–µ–±-–∫–∞–º–µ—Ä–∞
elif option == "–í–µ–±-–∫–∞–º–µ—Ä–∞":
    class VideoProcessor(VideoTransformerBase):
        def transform(self, frame):
            img = frame.to_ndarray(format="bgr24")
            results = model(img)
            rendered = np.squeeze(results.render())
            return cv2.cvtColor(rendered, cv2.COLOR_RGB2BGR)

    st.write("–ù–∞–∂–º–∏—Ç–µ 'Start' –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤–µ–±-–∫–∞–º–µ—Ä—ã")
    webrtc_streamer(key="webcam", video_processor_factory=VideoProcessor)
